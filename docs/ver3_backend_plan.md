# Face Search Backend (Model v3) – Packaging Plan

## 1. Objectives
- Expose model v3 through a FastAPI service that wraps Triton inference and delivers face detection plus embedding extraction only.
- Accept multiple images per request, detect faces, align them, and return 1024-d embeddings alongside detection metadata needed by downstream services.
- Offer a dedicated rerank endpoint that operates purely on embeddings so the existing backend web API can call Milvus independently and then request rerank scores.
- Maintain high throughput by delegating heavy computation to Triton and using the existing C++ rerank shared library through a Python wrapper.

## 2. High-Level Architecture
- **FastAPI App** – Hosts REST endpoints, handles request validation, orchestrates inference or rerank calls, and returns structured JSON.
- **Triton Client Layer** – Adaptation of the `StandaloneTritonClient` from `face_v3/infer.py`, reused via dependency injection and thread pools for async friendliness.
- **Rerank Service** – Python wrapper around `librerank_compute.so`, instantiated once at startup and reused for scoring candidate lists supplied by upstream services.
- **Response Builder** – Packages per-face detection metadata, aligned face info (optional), embeddings, and rerank scores in a consistent schema.
- **Deployment** – Docker image containing the FastAPI app, Triton gRPC client dependencies, and required shared libraries (`libmerge_embeddings.so`, `librerank_compute.so`).

## 3. Request Workflows

### `/embeddings` Endpoint
1. **Receive Payload** – Accepts multiple images (multipart or base64). Optional flags: return aligned faces, skip embedding normalization, include raw detection boxes.
2. **Batch Decode & Preprocess** – Convert images to numpy arrays, validate limits, optionally deduplicate identical frames.
3. **Face Detection & Embedding** – Use Triton:
   - Detection ensemble returns bounding boxes and landmarks.
   - Align faces via similarity transform and send crops to the extraction ensemble for 1024-d embeddings.
4. **Response Assembly** – For each detected face, include bounding box, detection confidence, landmark positions, optional aligned crop (base64 or media link), and the embedding vector (optionally normalized).
5. **Return JSON** – Group results by input image so the web backend can pipe embeddings to Milvus or storage.

### `/rerank` Endpoint
1. **Receive Payload** – A query embedding and a list of candidate embeddings/IDs (typically from Milvus results) plus optional threshold override.
2. **Score** – Call the C++ rerank library to compute scores; apply thresholding to suppress weak matches.
3. **Return JSON** – Provide rerank scores (0–1 range) tagged with caller-supplied candidate IDs and optional diagnostics (latency, threshold used).

## 4. FastAPI Service Layout
- `app/main.py` – FastAPI initialization, dependency wiring, startup/shutdown events.
- `app/api/v1/routes.py` – Defines `/embeddings`, `/rerank`, and auxiliary endpoints (e.g., `/healthz`).
- `app/services/triton_client.py` – gRPC client for detection/extraction with batching helpers and pooled executors.
- `app/services/rerank.py` – Singleton wrapper around `librerank_compute.so`, exposing `compute_scores`.
- `app/core/config.py` – Environment-driven settings (Triton URL, rerank thresholds, batch limits, optional flags).
- `app/schemas.py` – Pydantic models for requests/responses to power autogenerated docs and validation.
- `app/utils/image.py` – Image decoding, alignment, chunking, and normalization utilities reused from `face_v3/infer.py`.

## 5. Rerank Integration Details
- Load `RerankComputeCpp` once at startup; fail fast if the shared object cannot be loaded.
- Support per-request threshold overrides while providing a default via configuration for consistency across environments.
- Expose latency metrics and candidate counts for observability; include optional debug payload when callers opt in.
- Keep rerank stateless so multiple API pods can scale horizontally without coordination.

## 6. Concurrency & Performance
- Run FastAPI with Uvicorn workers; use batching in Triton calls to amortize RPC overhead when multiple faces/images arrive together.
- Offload blocking Triton gRPC calls to a `ThreadPoolExecutor` (or similar) to maintain async responsiveness.
- Cache Triton model metadata at startup and reuse channels to avoid repeated handshake costs.
- Monitor rerank execution time; expect sub-millisecond latency per request for typical candidate counts (<200).

## 7. Deployment Plan
- **Dockerfile**
  - Base on a CUDA-enabled Python image compatible with the Triton gRPC client.
  - Install system libraries required by OpenCV and the shared objects.
  - Copy the FastAPI application, Python dependencies, and shared libraries (`libmerge_embeddings.so`, `librerank_compute.so`).
  - Expose port 8000; run `uvicorn app.main:app --host 0.0.0.0 --port 8000`.
- **Configuration**
  - Environment variables for `TRITON_URL`, `RERANK_THRESHOLD`, `MAX_BATCH_SIZE`, `RETURN_ALIGNED`.
  - Optionally mount directories for shared libraries or face alignment templates if they change per deployment.
- **Orchestration**
  - Provide Kubernetes manifest or docker-compose snippet for running the FastAPI service alongside Triton in staging/production.
  - `/healthz` checks should verify Triton connectivity and rerank library availability; optionally include a lightweight inference smoke test.

## 8. Testing Strategy
- Unit tests for image decoding/alignment utilities and rerank wrapper initialization.
- Integration tests with mocked Triton responses to validate embedding payloads and error paths.
- Contract tests for `/rerank` using deterministic embeddings to confirm threshold behavior.
- Load tests sending multi-image batches to measure end-to-end latency and throughput.
- Manual verification with `face_v3/images_test` to confirm detection counts, embedding quality, and rerank outputs.

## 9. Rollout Steps
1. Extract reusable utilities from `face_v3/infer.py` into sharable modules within the FastAPI project.
2. Implement FastAPI skeleton and wire startup dependencies (Triton client, rerank wrapper).
3. Build Docker image locally; confirm shared libraries load and sample requests succeed.
4. Deploy to staging with Triton v3 server; run regression tests for embedding accuracy and rerank scoring.
5. Update API documentation and client usage notes before promoting to production.
6. Monitor logs/metrics post-release; tune rerank threshold and batching parameters as needed.

