# Face Search Backend (Model v3) – Packaging Plan

## 1. Objectives
- Expose model v3 through a FastAPI service that wraps Triton inference and delivers face detection plus embedding extraction only.
- Accept multiple images per request, detect faces, align them, and return 1024-d embeddings alongside detection metadata needed by downstream services.
- Offer a dedicated rerank endpoint that operates purely on embeddings so the existing backend web API can call Milvus independently and then request rerank scores.
- Maintain high throughput by delegating heavy computation to Triton and using the existing C++ rerank shared library through a Python wrapper.

## 2. High-Level Architecture
- **FastAPI App** – Hosts REST endpoints, handles request validation, orchestrates inference or rerank calls, and returns structured JSON.
- **Triton Client Layer** – Adaptation of the `StandaloneTritonClient` from `face_v3/infer.py`, reused via dependency injection and thread pools for async friendliness.
- **Rerank Service** – Python wrapper around `librerank_compute.so`, instantiated once at startup and reused for scoring candidate lists supplied by upstream services.
- **Response Builder** – Packages per-face detection metadata, aligned face info (optional), embeddings, and rerank scores in a consistent schema.
- **Deployment** – Docker image for the FastAPI app orchestrated via docker-compose alongside a Triton Server container built from `triton-infer-custom/` assets, sharing a network and any required model/shared-library volumes (`libmerge_embeddings.so`, `librerank_compute.so`).

## 3. Request Workflows

### `/embeddings` Endpoint
1. **Receive Payload** – Accepts multiple images (multipart or base64). Optional flags: return aligned faces, skip embedding normalization, include raw detection boxes.
2. **Batch Decode & Preprocess** – Convert images to numpy arrays, validate limits, optionally deduplicate identical frames.
3. **Face Detection & Embedding** – Use Triton:
   - Detection ensemble returns bounding boxes and landmarks.
   - Align faces via similarity transform and send crops to the extraction ensemble for 1024-d embeddings.
4. **Response Assembly** – For each detected face, include bounding box, detection confidence, landmark positions, optional aligned crop (base64 or media link), and the embedding vector (optionally normalized).
5. **Return JSON** – Group results by input image so the web backend can pipe embeddings to Milvus or storage.

### `/rerank` Endpoint
1. **Receive Payload** – A query embedding and a list of candidate embeddings/IDs (typically from Milvus results) plus optional threshold override.
2. **Score** – Call the C++ rerank library to compute scores; apply thresholding to suppress weak matches.
3. **Return JSON** – Provide rerank scores (0–1 range) tagged with caller-supplied candidate IDs and optional diagnostics (latency, threshold used).

## 4. FastAPI Service Layout
- `app/main.py` – FastAPI initialization, dependency wiring, startup/shutdown events.
- `app/api/v1/routes.py` – Defines `/embeddings`, `/rerank`, and auxiliary endpoints (e.g., `/healthz`).
- `app/services/triton_client.py` – gRPC client for detection/extraction with batching helpers and pooled executors.
- `app/services/rerank.py` – Singleton wrapper around `librerank_compute.so`, exposing `compute_scores`.
- `app/core/config.py` – Environment-driven settings (Triton URL, rerank thresholds, batch limits, optional flags).
- `app/schemas.py` – Pydantic models for requests/responses to power autogenerated docs and validation.
- `app/utils/image.py` – Image decoding, alignment, chunking, and normalization utilities reused from `face_v3/infer.py`.

## 5. Rerank Integration Details
- Load `RerankComputeCpp` once at startup; fail fast if the shared object cannot be loaded.
- Support per-request threshold overrides while providing a default via configuration for consistency across environments.
- Expose latency metrics and candidate counts for observability; include optional debug payload when callers opt in.
- Keep rerank stateless so multiple API pods can scale horizontally without coordination.

## 6. Concurrency & Performance
- Run FastAPI with Uvicorn workers; use batching in Triton calls to amortize RPC overhead when multiple faces/images arrive together.
- Offload blocking Triton gRPC calls to a `ThreadPoolExecutor` (or similar) to maintain async responsiveness.
- Cache Triton model metadata at startup and reuse channels to avoid repeated handshake costs.
- Monitor rerank execution time; expect sub-millisecond latency per request for typical candidate counts (<200).

## 7. Deployment Plan
- **Dockerfile (API Service)**
  - Base on a CUDA-enabled Python image compatible with the Triton gRPC client.
  - Install system libraries required by OpenCV and the shared objects.
  - Copy the FastAPI application, Python dependencies, and shared libraries (`libmerge_embeddings.so`, `librerank_compute.so`).
  - Expose port 8000; run `uvicorn app.main:app --host 0.0.0.0 --port 8000`.
- **Docker Compose Stack**
  - Define `docker-compose.yml` with two services:
    - `api`: builds from the local Dockerfile, mounts shared libraries if they are distributed outside the image, and injects environment variables (`TRITON_URL=triton:8001`, `RERANK_THRESHOLD`, `MAX_BATCH_SIZE`, `RETURN_ALIGNED`).
    - `triton`: references the custom image tag produced by `triton-infer-custom/build.sh` (defaults to `tritonserver-plan-vnd:24.11-py3`), mounts the generated TensorRT engines under `models_serving`, and exposes gRPC on `8001` (left un-published but available on the compose network).
  - Use a dedicated Docker network created by compose so the API refers to Triton via the service name (`triton`).
  - Mount a shared volume if the API must read inference outputs (e.g., cached templates) written by Triton, otherwise keep services isolated.
  - Configure `depends_on` with health checks so the API waits for Triton readiness before starting; reuse Triton health endpoint or add a custom script that polls gRPC readiness.
- **Triton Build Flow**
  - Run `triton-infer-custom/build.sh` to convert ONNX models to TensorRT engines (`model.plan`) before building the `tritonserver-plan-vnd:24.11-py3` image. The script currently requires local GPU access and Docker with NVIDIA runtime for `trtexec`.
  - The accompanying Dockerfile installs CUDA-compatible dependencies, copies `models_serving`, and installs the custom Python ops (`pip3 install /models/split_b/.../rpe_ops`) prior to launching Triton. Update the script if additional models or dependencies are introduced.
  - Document any environment prerequisites (sudo-less docker, GPU access) and consider wrapping the build in Make targets for reproducibility.
- **Configuration & Secrets**
  - Provide a `.env` file consumed by docker-compose to centralize environment knobs for both services (thresholds, batching, log levels, optional tracing endpoints).
  - Allow overrides for GPU selection (e.g., `device_requests` in compose) and optional host mounts for observability exporters.
- **Operational Notes**
  - `/healthz` should validate Triton connectivity and rerank library availability at startup.
  - For staging/production, extend the compose file with profiles for metrics/monitoring sidecars or adapt it into Kubernetes manifests once requirements outgrow single-host deployment.

## 8. Testing Strategy
- Unit tests for image decoding/alignment utilities and rerank wrapper initialization.
- Integration tests with mocked Triton responses to validate embedding payloads and error paths.
- Contract tests for `/rerank` using deterministic embeddings to confirm threshold behavior.
- Load tests sending multi-image batches to measure end-to-end latency and throughput.
- Manual verification with `face_v3/images_test` to confirm detection counts, embedding quality, and rerank outputs.

## 9. Rollout Steps
1. Extract reusable utilities from `face_v3/infer.py` into sharable modules within the FastAPI project.
2. Implement FastAPI skeleton and wire startup dependencies (Triton client, rerank wrapper).
3. Build Docker image locally; confirm shared libraries load and sample requests succeed.
4. Deploy to staging with Triton v3 server; run regression tests for embedding accuracy and rerank scoring.
5. Update API documentation and client usage notes before promoting to production.
6. Monitor logs/metrics post-release; tune rerank threshold and batching parameters as needed.
